{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcTwzhX8fBqs"
      },
      "source": [
        "Code based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "\n",
        "In this exercise, we are going to implement a [ResNet-like](https://arxiv.org/pdf/1512.03385.pdf) architecture for the image classification task.\n",
        "The model is trained on the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "    1. Implement residual connections in the missing places in the code.\n",
        "\n",
        "    2. Check that the given implementation reaches 97% test accuracy after a few epochs.\n",
        "\n",
        "    3. Check that when extending the residual blocks to 20 (having 40+ layers total), the model still trains well, i.e., achieves 97+% accuracy after three epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IYAsziKffBFV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        # TODO: implement constructor\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=out_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: implement forward function\n",
        "        identity = x\n",
        "        x = self.conv_block_1(x)\n",
        "        x = self.conv_block_2(x)\n",
        "\n",
        "        x += identity\n",
        "        x = nn.ReLU()(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.rc = nn.Sequential(\n",
        "            ResidualConnection(1, 16),\n",
        "            *(ResidualConnection(16, 16) for _ in range(19)),\n",
        "        )\n",
        "        self.fc = nn.Linear(\n",
        "            28 * 28 * 16, 10\n",
        "        )  # 28 * 28 * 16 is the size of flattened output of the last ResidualConnection\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rc(x)\n",
        "        x = nn.Flatten(start_dim=1)(x)\n",
        "        x = self.fc(x)\n",
        "        output = nn.LogSoftmax(dim=1)(x)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DMtap4QCfBH8"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(\n",
        "                output, target, reduction=\"sum\"\n",
        "            ).item()  # sum up batch loss\n",
        "            pred = output.argmax(\n",
        "                dim=1, keepdim=True\n",
        "            )  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss,\n",
        "            correct,\n",
        "            len(test_loader.dataset),\n",
        "            100.0 * correct / len(test_loader.dataset),\n",
        "        )\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K5GlMs1-fBKP"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "test_batch_size = 1000\n",
        "epochs = 3\n",
        "lr = 1e-2\n",
        "use_cuda = False\n",
        "seed = 1\n",
        "log_interval = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WgfUP23AfBMd"
      },
      "outputs": [],
      "source": [
        "use_cuda = not use_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "train_kwargs = {\"batch_size\": batch_size}\n",
        "test_kwargs = {\"batch_size\": test_batch_size}\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0KPoUtsfBOs",
        "outputId": "4ee308b0-0aac-4d3c-f372-352f28970104"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        ")\n",
        "dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
        "dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezvIQbgsfBRT",
        "outputId": "3f6621ef-0bad-46c6-bd8f-ac535db8e9af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.761839\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 40.722206\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 3.639362\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 6.367097\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.505010\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 3.670895\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.411293\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.616078\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.794972\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.584466\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.763387\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.662184\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.605093\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.719983\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.692101\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.958127\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.393382\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.412869\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.546433\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.362485\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.226467\n",
            "Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.257889\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.355420\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.037470\n",
            "\n",
            "Test set: Average loss: 0.2431, Accuracy: 9334/10000 (93%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.278333\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.181748\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.266307\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.235784\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.260674\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.265041\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.188047\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 1.064675\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.189307\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.150731\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.191187\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.286476\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.208917\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.204539\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.186337\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.123620\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.119779\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.109667\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.167823\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.097986\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.083570\n",
            "Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.149516\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.128180\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.009305\n",
            "\n",
            "Test set: Average loss: 0.1547, Accuracy: 9623/10000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.125863\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.109296\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.146115\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.170959\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.187800\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.139319\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.111237\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.087229\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.109058\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.064097\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.137497\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.129740\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.079264\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.146104\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.092025\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.070248\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.074844\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.090239\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.104471\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.053408\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.057120\n",
            "Train Epoch: 3 [53760/60000 (89%)]\tLoss: 0.085151\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.080287\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.006539\n",
            "\n",
            "Test set: Average loss: 0.0823, Accuracy: 9728/10000 (97%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
        "    test(model, device, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "6fa2fa4f4d9d3d9ca73eb3739cc0e85a72773041ed8c7376d5dc2c41e6946bf8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
