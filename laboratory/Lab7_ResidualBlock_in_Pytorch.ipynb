{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcTwzhX8fBqs"
      },
      "source": [
        "Code based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "\n",
        "In this exercise, we are going to implement a [ResNet-like](https://arxiv.org/pdf/1512.03385.pdf) architecture for the image classification task.\n",
        "The model is trained on the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "    1. Implement residual connections in the missing places in the code.\n",
        "\n",
        "    2. Check that the given implementation reaches 97% test accuracy after a few epochs.\n",
        "\n",
        "    3. Check that when extending the residual blocks to 20 (having 40+ layers total), the model still trains well, i.e., achieves 97+% accuracy after three epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYAsziKffBFV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7e1oczdkXc-"
      },
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=out_channels,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=3,\n",
        "                padding=1,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: implement forward pass.\n",
        "        ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGhTsGO5kXc_"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.rc = nn.Sequential(\n",
        "            ResidualConnection(1, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "            ResidualConnection(16, 16),\n",
        "        )\n",
        "        self.fc = nn.Linear(\n",
        "            28 * 28 * 16, 10\n",
        "        )  # 28 * 28 * 16 is the size of flattened output of the last ResidualConnection\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rc(x)\n",
        "        x = nn.Flatten(start_dim=1)(x)\n",
        "        x = self.fc(x)\n",
        "        output = nn.LogSoftmax(dim=1)(x)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMtap4QCfBH8"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(\n",
        "                output, target, reduction=\"sum\"\n",
        "            ).item()  # sum up batch loss\n",
        "            pred = output.argmax(\n",
        "                dim=1, keepdim=True\n",
        "            )  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss,\n",
        "            correct,\n",
        "            len(test_loader.dataset),\n",
        "            100.0 * correct / len(test_loader.dataset),\n",
        "        )\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5GlMs1-fBKP"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "test_batch_size = 1000\n",
        "epochs = 3\n",
        "lr = 1e-2\n",
        "use_cuda = False\n",
        "seed = 1\n",
        "log_interval = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgfUP23AfBMd"
      },
      "outputs": [],
      "source": [
        "use_cuda = not use_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "train_kwargs = {\"batch_size\": batch_size}\n",
        "test_kwargs = {\"batch_size\": test_batch_size}\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0KPoUtsfBOs"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        ")\n",
        "dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n",
        "dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezvIQbgsfBRT",
        "outputId": "3f6621ef-0bad-46c6-bd8f-ac535db8e9af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.619462\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 11.626164\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.870938\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.584417\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.896171\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.962128\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.598487\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.387887\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.483061\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.248355\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.253035\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.307905\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.206146\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.192347\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.303769\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.167431\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.216710\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.137700\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.245606\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.076815\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.121641\n",
            "Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.127982\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.117696\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.008412\n",
            "\n",
            "Test set: Average loss: 0.1267, Accuracy: 9655/10000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.187387\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.151279\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.156193\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.079677\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.080873\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.082624\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.136585\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.126576\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.133855\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.088517\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.137870\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.139044\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.090397\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.156203\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.132147\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.064799\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.110945\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.061094\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.119679\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.041621\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.065102\n",
            "Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.040525\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.080363\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.002100\n",
            "\n",
            "Test set: Average loss: 0.0816, Accuracy: 9757/10000 (98%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
        "    test(model, device, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQMSSwuifBTo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX_2rCycfBWU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "6fa2fa4f4d9d3d9ca73eb3739cc0e85a72773041ed8c7376d5dc2c41e6946bf8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}