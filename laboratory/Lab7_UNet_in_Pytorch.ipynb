{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Code based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "\n",
        "In this exercise, we are going to implement a [UNet-like](https://arxiv.org/pdf/1505.04597.pdf) architecture for the semantic segmentation task. \n",
        "The model is trained on the [Pascal VOC](https://paperswithcode.github.io/torchbench/pascalvoc/) dataset.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "    1. Implement the missing pieces in the code.\n",
        "\n",
        "    2. Check that the given implementation reaches 68% test accuracy after a few epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QfIXmJ-dRXfE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FxGS_WsORXfF"
      },
      "outputs": [],
      "source": [
        "class UNetConvolutionStack(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(UNetConvolutionStack, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channel),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lyN2g-yQRXfG"
      },
      "outputs": [],
      "source": [
        "class EncoderStack(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, first_layer=False):\n",
        "        super(EncoderStack, self).__init__()\n",
        "        if first_layer:\n",
        "            self.down = nn.Sequential(\n",
        "                UNetConvolutionStack(in_channel, out_channel),\n",
        "                UNetConvolutionStack(out_channel, out_channel),\n",
        "            )\n",
        "        else:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.MaxPool2d((2, 2)),\n",
        "                UNetConvolutionStack(in_channel, out_channel),\n",
        "                UNetConvolutionStack(out_channel, out_channel),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.down(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dp2-OwXORXfG"
      },
      "outputs": [],
      "source": [
        "class DecoderStack(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(DecoderStack, self).__init__()\n",
        "        self.upsample = nn.ConvTranspose2d(\n",
        "            in_channel, in_channel, 3, stride=2, padding=1\n",
        "        )\n",
        "        self.up = nn.Sequential(\n",
        "            UNetConvolutionStack(in_channel + out_channel, out_channel),\n",
        "            UNetConvolutionStack(out_channel, out_channel),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # TODO: implement skipconnections.\n",
        "        # hint: x is the output of previous decoder layer,\n",
        "        # y is the output of corresponding encoder layer.\n",
        "        # Based on the arguments of the constructor,\n",
        "        # how should x and y be combined?\n",
        "        x = self.upsample(x, output_size=y.size())\n",
        "        x = torch.cat([y, x], dim=1)\n",
        "        x = self.up(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RBPeqMNSRXfG"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, encoder_channels, decoder_channels, num_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.encoder = nn.ModuleList()\n",
        "        self.decoder = nn.ModuleList()\n",
        "        self.conv = nn.Conv2d(\n",
        "            decoder_channels[-1], num_classes, kernel_size=3, padding=1\n",
        "        )\n",
        "\n",
        "        encoder_sizes = zip(\n",
        "            range(len(encoder_channels)), encoder_channels, encoder_channels[1:]\n",
        "        )\n",
        "        for idx, in_size, out_size in encoder_sizes:\n",
        "            if idx > 0:\n",
        "                self.encoder.append(EncoderStack(in_size, out_size))\n",
        "            else:\n",
        "                self.encoder.append(EncoderStack(in_size, out_size, first_layer=True))\n",
        "\n",
        "        decoder_sizes = zip(decoder_channels, decoder_channels[1:])\n",
        "        for in_size, out_size in decoder_sizes:\n",
        "            self.decoder.append(DecoderStack(in_size, out_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: implement UNet's forward pass.\n",
        "        # hint: Remeber to store outputs of subsequent\n",
        "        # encoder layers to use as input to decoer layers!\n",
        "        # Do not forget about the final convolution.\n",
        "        encoded_layers = []\n",
        "        for e in self.encoder:\n",
        "            x = e(x)\n",
        "            encoded_layers.append(x)\n",
        "\n",
        "        del encoded_layers[-1]\n",
        "        encoded_layers.reverse()\n",
        "\n",
        "        for y, d in zip(encoded_layers, self.decoder):\n",
        "            x = d(x, y)\n",
        "\n",
        "        x = self.conv(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5AKH3oUqRXfH"
      },
      "outputs": [],
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(\n",
        "                output, target, reduction=\"sum\"\n",
        "            ).item()  # sum up batch loss\n",
        "            pred = output.argmax(\n",
        "                dim=1, keepdim=True\n",
        "            )  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    _, _, image_width, image_height = data.size()\n",
        "    test_loss /= len(test_loader.dataset) * image_width * image_height\n",
        "\n",
        "    print(\n",
        "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss,\n",
        "            correct,\n",
        "            (len(test_loader.dataset) * image_width * image_height),\n",
        "            100.0 * correct / (len(test_loader.dataset) * image_width * image_height),\n",
        "        )\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ed1Rwhv-RXfH"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "test_batch_size = 1000\n",
        "epochs = 5\n",
        "lr = 1e-2\n",
        "use_cuda = True\n",
        "seed = 1\n",
        "log_interval = 10\n",
        "\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std = [0.229, 0.224, 0.225]\n",
        "num_classes = 22\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ht3SPPVlRXfH"
      },
      "outputs": [],
      "source": [
        "use_cuda = use_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "train_kwargs = {\"batch_size\": batch_size}\n",
        "test_kwargs = {\"batch_size\": test_batch_size}\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dD85qSzwRXfI"
      },
      "outputs": [],
      "source": [
        "def replace_tensor_value_(tensor, a, b):\n",
        "    tensor[tensor == a] = b\n",
        "    return tensor\n",
        "\n",
        "\n",
        "input_resize = transforms.Resize((224, 224))\n",
        "input_transform = transforms.Compose(\n",
        "    [\n",
        "        input_resize,\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
        "    ]\n",
        ")\n",
        "\n",
        "target_resize = transforms.Resize((224, 224), interpolation=InterpolationMode.NEAREST)\n",
        "target_transform = transforms.Compose(\n",
        "    [\n",
        "        target_resize,\n",
        "        transforms.PILToTensor(),\n",
        "        transforms.Lambda(\n",
        "            lambda x: replace_tensor_value_(x.squeeze(0).long(), 255, 21)\n",
        "        ),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7r9wpzeRXfI",
        "outputId": "2b76544d-e235-45f1-fe29-e96972a780a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: ../data/VOCtrainval_11-May-2012.tar\n",
            "Extracting ../data/VOCtrainval_11-May-2012.tar to ../data\n",
            "Using downloaded and verified file: ../data/VOCtrainval_11-May-2012.tar\n",
            "Extracting ../data/VOCtrainval_11-May-2012.tar to ../data\n"
          ]
        }
      ],
      "source": [
        "dataset1 = datasets.VOCSegmentation(\n",
        "    \"../data\",\n",
        "    year=\"2012\",\n",
        "    image_set=\"train\",\n",
        "    download=True,\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform,\n",
        ")\n",
        "dataset2 = datasets.VOCSegmentation(\n",
        "    \"../data\",\n",
        "    year=\"2012\",\n",
        "    image_set=\"val\",\n",
        "    download=True,\n",
        "    transform=input_transform,\n",
        "    target_transform=target_transform,\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **train_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1KtqXIPRXfJ",
        "outputId": "226945b4-4a1a-4217-82a8-ee4094dd1562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/1464 (0%)]\tLoss: 0.043251\n",
            "Train Epoch: 1 [640/1464 (43%)]\tLoss: -3.852600\n",
            "Train Epoch: 1 [1280/1464 (87%)]\tLoss: -9.400640\n",
            "\n",
            "Test set: Average loss: -14.5673, Accuracy: 31287547/72705024 (43%)\n",
            "\n",
            "Train Epoch: 2 [0/1464 (0%)]\tLoss: -11.451077\n",
            "Train Epoch: 2 [640/1464 (43%)]\tLoss: -20.038708\n",
            "Train Epoch: 2 [1280/1464 (87%)]\tLoss: -31.201717\n",
            "\n",
            "Test set: Average loss: -28.1806, Accuracy: 22038447/72705024 (30%)\n",
            "\n",
            "Train Epoch: 3 [0/1464 (0%)]\tLoss: -35.037285\n",
            "Train Epoch: 3 [640/1464 (43%)]\tLoss: -50.716175\n",
            "Train Epoch: 3 [1280/1464 (87%)]\tLoss: -71.475281\n",
            "\n",
            "Test set: Average loss: -71.9071, Accuracy: 37214447/72705024 (51%)\n",
            "\n",
            "Train Epoch: 4 [0/1464 (0%)]\tLoss: -78.269875\n",
            "Train Epoch: 4 [640/1464 (43%)]\tLoss: -103.790581\n",
            "Train Epoch: 4 [1280/1464 (87%)]\tLoss: -134.241089\n",
            "\n",
            "Test set: Average loss: -141.6039, Accuracy: 50330809/72705024 (69%)\n",
            "\n",
            "Train Epoch: 5 [0/1464 (0%)]\tLoss: -144.246964\n",
            "Train Epoch: 5 [640/1464 (43%)]\tLoss: -179.942841\n",
            "Train Epoch: 5 [1280/1464 (87%)]\tLoss: -221.873611\n",
            "\n",
            "Test set: Average loss: -233.7585, Accuracy: 50341167/72705024 (69%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model = UNet(\n",
        "    encoder_channels=[3, 8, 16, 32],\n",
        "    decoder_channels=[32, 16, 8],\n",
        "    num_classes=num_classes,\n",
        ").to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
        "    test(model, device, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GifpCp-rRXfJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Y2Aa03GTRXfJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "6fa2fa4f4d9d3d9ca73eb3739cc0e85a72773041ed8c7376d5dc2c41e6946bf8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
