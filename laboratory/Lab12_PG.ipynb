{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39250c4d",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "In reinforcement learning, the goal is to find a policy that maximizes the expected cumulative reward over time. Policy Gradient (PG) methods are a class of algorithms for finding such policies in which the policy is represented by a parameterized function, such as a neural network. The key idea behind PG methods is to adjust the parameters of the policy in the direction that increases the expected reward. This is done by computing the gradient of the expected reward with respect to the policy parameters and updating the parameters in the direction of the gradient.\n",
    "\n",
    "Unfortunately, calculating exact policy gradient requires iterating over the entire state-action space, which is unfeasible in most problems. As such, we estimate the gradient with a sample of recent transitions, which were sampled according to the policy. In its simplest form, PG is equal to:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} J = \\underset{s \\sim p^{\\pi}_{*}}{\\mathbb{E}} ~ \\underset{a \\sim \\pi}{\\mathbb{E}} ~ Q^{\\pi} (s, a) ~ \\nabla_{\\theta} \\log \\pi_{\\theta} (a | s)\n",
    "$$\n",
    "\n",
    "Where $p^{\\pi}_{*}$ is the discounted stationary distribution, $\\pi_{\\theta}(a|s)$ is the policy and $Q^{\\pi} (s, a)$ denotes the Q-value under policy. Some of the popular policy gradient algorithm are Actor-Critic and PPO. These algorithm use different techniques to estimate the gradient and update the policy.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Run the **PolicyGradient** agent. Plot mean+std of rewards during the training.\n",
    "2. Implement the *update* method for **BaselinedPolicyGradient** class. Run the agent, plot mean+std of rewards during the training.\n",
    "3. Implement the *update* method for **ActorCritic** class. Run the agent, plot mean+std of rewards during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acf50b2",
   "metadata": {},
   "source": [
    "We import required modules and fix hyperparamters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ceeec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f47d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change!\n",
    "class parse_args:\n",
    "    def __init__(self):\n",
    "        self.gym_id = \"CartPole-v0\"\n",
    "        self.batch_size = 32\n",
    "        self.hidden_dim = 32\n",
    "        self.learning_rate = 5e-3\n",
    "        self.discount = 0.99\n",
    "        self.timesteps = 10000\n",
    "        self.repeats = 10\n",
    "        self.eval_steps = 500\n",
    "        self.cuda = True\n",
    "        env = gym.make(self.gym_id)\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and self.cuda else \"cpu\")\n",
    "\n",
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a2a605",
   "metadata": {},
   "source": [
    "We define a policy network (**ActorNetwork**) and a value network (**CriticNetwork**). Value network has multiple uses, but we will initially use it to bootstrap Q-values of unfinished episodes.\n",
    "\n",
    "Why do we need such bootstrap? Our environment can run up to 200 steps before termination. Given batch size of 32, we will never be able to get entire episode in one batch. As PG requires Q-values of all state-actions in the batch, we have to find a way of calculating the Q-values past our batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d97a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(args.state_size, args.hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(args.hidden_dim, args.action_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return nn.Softmax(1)(self.layers(x))\n",
    "    \n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(args.state_size, args.hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(args.hidden_dim, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183095c8",
   "metadata": {},
   "source": [
    "Below, we implement the PG agent. There are two important methods here:\n",
    "\n",
    "1. *get_qvalues* - calculate Q-values for the batch. If final state is not terminating, use value network to estimate value of past final state.\n",
    "2. *update* - update actor and citic networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "585f0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(PolicyGradient, self).__init__()\n",
    "        self.args = args\n",
    "        self.init_networks()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.logprobs = torch.zeros(self.args.batch_size)\n",
    "        self.values = torch.zeros(self.args.batch_size)\n",
    "        self.rewards = torch.zeros(self.args.batch_size)\n",
    "        self.terminals = torch.zeros(self.args.batch_size)\n",
    "        self.idx = 0\n",
    "        \n",
    "    def init_networks(self):\n",
    "        self.actor = ActorNetwork(self.args)\n",
    "        self.critic = CriticNetwork(self.args)\n",
    "        self.optimizer = optim.Adam(list(self.actor.parameters()) + list(self.critic.parameters()), lr=self.args.learning_rate)\n",
    "        self.reset()\n",
    "\n",
    "    def add(self, logprob, value, reward, terminal):\n",
    "        self.logprobs[self.idx] = logprob\n",
    "        self.values[self.idx] = value\n",
    "        self.rewards[self.idx] = reward\n",
    "        self.terminals[self.idx] = terminal\n",
    "        self.idx += 1\n",
    "\n",
    "    def get_action_and_value(self, x):\n",
    "        probs = self.actor(x)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        return action.item(), dist.log_prob(action), self.critic(x)\n",
    "\n",
    "    def get_qvalues(self, final_state):\n",
    "        with torch.no_grad():\n",
    "            final_state_value = self.critic(final_state)\n",
    "        q_values = torch.zeros_like(self.rewards).to(self.args.device)\n",
    "        for timestep in reversed(range(self.args.batch_size)):\n",
    "            next_value = final_state_value if timestep == (self.args.batch_size - 1) else q_values[timestep + 1]\n",
    "            q_values[timestep] = self.rewards[timestep] + self.args.discount * (1 - self.terminals[timestep]) * next_value\n",
    "        return q_values\n",
    "    \n",
    "    def update(self, q_values):\n",
    "        policy_loss = -(self.logprobs * q_values).mean()\n",
    "        value_loss = (self.values - q_values).pow(2).mean()\n",
    "        loss = (policy_loss + 0.5*value_loss)\n",
    "        self.optimizer.zero_grad()  \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.reset()\n",
    "        \n",
    "    def evaluate(self, samples):\n",
    "        with torch.no_grad():\n",
    "            env_test = gym.make(self.args.gym_id)\n",
    "            eval_reward = 0\n",
    "            for i in range(samples):\n",
    "                state = env_test.reset()\n",
    "                episode_reward = 0\n",
    "                while True:\n",
    "                    action, _, _ = self.get_action_and_value(torch.tensor(state).unsqueeze(0))\n",
    "                    next_state, reward, terminal, _ = env_test.step(action)\n",
    "                    episode_reward += reward\n",
    "                    state = next_state\n",
    "                    if terminal:\n",
    "                        eval_reward += episode_reward/samples\n",
    "                        break\n",
    "        return eval_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90597872",
   "metadata": {},
   "source": [
    "Finally, a function to train the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc0d5af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, agent):\n",
    "    rewards = np.zeros((args.timesteps//args.eval_steps, args.repeats))\n",
    "    for seed in range(args.repeats):\n",
    "        env = gym.make(args.gym_id)\n",
    "        agent.init_networks()\n",
    "        state = env.reset()\n",
    "        start_time = time.time()\n",
    "        for step in range(args.timesteps):\n",
    "            action, logprob, value = agent.get_action_and_value(torch.tensor(state).unsqueeze(0))\n",
    "            next_state, reward, terminal, _ = env.step(action)\n",
    "            agent.add(logprob, value, reward, terminal)\n",
    "            state = next_state\n",
    "            if agent.idx == args.batch_size:\n",
    "                q_values = agent.get_qvalues(torch.tensor(state).unsqueeze(0))\n",
    "                agent.update(q_values)\n",
    "            if (step+1) % args.eval_steps == 0:\n",
    "                eval_rewards = agent.evaluate(5)\n",
    "                rewards[step//args.eval_steps, seed] = eval_rewards\n",
    "                if (step+1) % (10 * args.eval_steps) == 0:\n",
    "                    print(f\"step={step+1}, eval_return={np.round(eval_rewards, 1)}, samples_per_sec={int(step / (time.time() - start_time))}\")\n",
    "            if terminal:\n",
    "                state = env.reset()\n",
    "                episode_reward = 0\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e625e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=5000, eval_return=87.4, samples_per_sec=1547\n",
      "step=10000, eval_return=134.4, samples_per_sec=1451\n",
      "step=5000, eval_return=86.0, samples_per_sec=1668\n",
      "step=10000, eval_return=198.6, samples_per_sec=1467\n",
      "step=5000, eval_return=73.2, samples_per_sec=1671\n",
      "step=10000, eval_return=34.6, samples_per_sec=1631\n",
      "step=5000, eval_return=62.0, samples_per_sec=1634\n",
      "step=10000, eval_return=93.4, samples_per_sec=1545\n",
      "step=5000, eval_return=40.4, samples_per_sec=1717\n",
      "step=10000, eval_return=199.6, samples_per_sec=1624\n",
      "step=5000, eval_return=136.2, samples_per_sec=1645\n",
      "step=10000, eval_return=178.0, samples_per_sec=1416\n",
      "step=5000, eval_return=46.0, samples_per_sec=1735\n",
      "step=10000, eval_return=105.6, samples_per_sec=1623\n",
      "step=5000, eval_return=69.6, samples_per_sec=1733\n",
      "step=10000, eval_return=112.2, samples_per_sec=1545\n",
      "step=5000, eval_return=52.4, samples_per_sec=1716\n",
      "step=10000, eval_return=91.8, samples_per_sec=1634\n",
      "step=5000, eval_return=100.8, samples_per_sec=1661\n",
      "step=10000, eval_return=184.0, samples_per_sec=1427\n"
     ]
    }
   ],
   "source": [
    "agent = PolicyGradient(args).to(args.device)\n",
    "rewards = train(args, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17abad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPH RESULTS HERE (MEAN + STD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f33e74",
   "metadata": {},
   "source": [
    "PG estimate has big variance - the estimate will vary a lot depending on the states and actions sampled. This variance is known to impact the sample efficiency and final performance of the agent. Although there are many strategies fo PG variance reduction, there is one trick that became indispensable in modern implementations of PG - baseline trick. But before we get to this, lets have one more look at the standard PG estimate:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} J = \\underset{s \\sim p^{\\pi}_{*}}{\\mathbb{E}} ~ \\underset{a \\sim \\pi}{\\mathbb{E}} ~ Q^{\\pi} (s, a) ~ \\nabla_{\\theta} \\log \\pi_{\\theta} (a | s)\n",
    "$$\n",
    "\n",
    "As mentioned in the lecture, PG nature is quite simple - increase the probability of good actions and decrease the probability of bad ones. 'Good' and 'bad' refer to Q-values associated with given actions. But what if all Q-values are positive? The gradient update will try to increase the probabilities of all sampled actions, with the increase being proportional to the Q-value (as stems from the equation above). As such, given positive Q-values, the probabilities of bad actions are also increased - just by a smaller amount that probabilities of actions with bigger Q-values. \n",
    "\n",
    "Baseline variance reduction tackles exactly that. The idea is to subtract a \"baseline\" from the Q-values, which does not affect the optimal policy, but reduces the variance of the gradients. It is proven that as long as the baseline does not depend on the action, its value will not bias the PG (however, inadequate baseline can actually increase the variance :)). So, what is a good baseline?\n",
    "\n",
    "The simplest version subtracts batch average of Q-values from each Q-value:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} J = \\sum_{i=1}^{B} \\frac{1}{B} \\bigl( Q^{\\pi} (s_i, a_i) - \\frac{\\sum Q^{\\pi} (s_i, a_i)}{B} \\bigr) \\nabla_{\\theta} \\log \\pi_{\\theta} (a_i | s_i)\n",
    "$$\n",
    "\n",
    "Where B is the batch size and $\\frac{\\sum Q^{\\pi} (s_i, a_i)}{B}$ is the average Q-value in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a145a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselinedPolicyGradient(PolicyGradient):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(BaselinedPolicyGradient, self).__init__(args)\n",
    "        \n",
    "    def update(self, q_values):\n",
    "        # your code here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bf6d9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=5000, eval_return=85.0, samples_per_sec=1679\n",
      "step=10000, eval_return=173.0, samples_per_sec=1477\n",
      "step=5000, eval_return=126.6, samples_per_sec=1641\n",
      "step=10000, eval_return=146.0, samples_per_sec=1421\n",
      "step=5000, eval_return=126.0, samples_per_sec=1629\n",
      "step=10000, eval_return=194.4, samples_per_sec=1480\n",
      "step=5000, eval_return=77.6, samples_per_sec=1766\n",
      "step=10000, eval_return=156.4, samples_per_sec=1592\n",
      "step=5000, eval_return=98.8, samples_per_sec=1693\n",
      "step=10000, eval_return=144.0, samples_per_sec=1490\n",
      "step=5000, eval_return=88.4, samples_per_sec=1724\n",
      "step=10000, eval_return=143.4, samples_per_sec=1509\n",
      "step=5000, eval_return=115.0, samples_per_sec=1532\n",
      "step=10000, eval_return=200.0, samples_per_sec=1326\n",
      "step=5000, eval_return=72.6, samples_per_sec=1505\n",
      "step=10000, eval_return=154.6, samples_per_sec=1444\n",
      "step=5000, eval_return=67.2, samples_per_sec=1750\n",
      "step=10000, eval_return=188.0, samples_per_sec=1494\n",
      "step=5000, eval_return=67.0, samples_per_sec=1772\n",
      "step=10000, eval_return=189.6, samples_per_sec=1476\n"
     ]
    }
   ],
   "source": [
    "agent = BaselinedPolicyGradient(args).to(args.device)\n",
    "rewards2 = train(args, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9811dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPH RESULTS HERE (MEAN + STD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f5f80",
   "metadata": {},
   "source": [
    "The most popular approach is to use state value as the baseline. Then, the gradient is well centered - probability of actions with Q-values smaller than state value will be decreased and probability of actions with Q-values bigger than state value will be increased. Obviously, we do not know the values associated with each state. To this end, we leverage a **CriticNetwork**. This network outputs value of a state and is trained using MSE using Q-values as targets. Traditionally, PG algorithm that uses value network for baseline variance reduction is called Actor-Critic. Actor Critic update is calculated with:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} J = \\sum_{i=1}^{B} \\frac{1}{B} \\bigl( Q^{\\pi} (s_i, a_i) - V_{\\phi}(s) \\bigr) \\nabla_{\\theta} \\log \\pi_{\\theta} (a_i | s_i)\n",
    "$$\n",
    "\n",
    "Where $\\bigl( Q^{\\pi} (s_i, a_i) - V_{\\phi}(s) \\bigr)$ is referred to as **advantage**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "261d7dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(PolicyGradient):\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        super(ActorCritic, self).__init__(args)\n",
    "        \n",
    "    def update(self, q_values):\n",
    "        # your code here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4df0c7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=5000, eval_return=63.0, samples_per_sec=1713\n",
      "step=10000, eval_return=183.8, samples_per_sec=1433\n",
      "step=5000, eval_return=68.0, samples_per_sec=1779\n",
      "step=10000, eval_return=184.2, samples_per_sec=1517\n",
      "step=5000, eval_return=83.0, samples_per_sec=1699\n",
      "step=10000, eval_return=89.6, samples_per_sec=1485\n",
      "step=5000, eval_return=148.8, samples_per_sec=1586\n",
      "step=10000, eval_return=200.0, samples_per_sec=1385\n",
      "step=5000, eval_return=121.4, samples_per_sec=1716\n",
      "step=10000, eval_return=198.0, samples_per_sec=1467\n",
      "step=5000, eval_return=109.4, samples_per_sec=1675\n",
      "step=10000, eval_return=159.0, samples_per_sec=1429\n",
      "step=5000, eval_return=97.8, samples_per_sec=1694\n",
      "step=10000, eval_return=186.2, samples_per_sec=1437\n",
      "step=5000, eval_return=109.6, samples_per_sec=1610\n",
      "step=10000, eval_return=176.0, samples_per_sec=1419\n",
      "step=5000, eval_return=104.6, samples_per_sec=1682\n",
      "step=10000, eval_return=166.6, samples_per_sec=1443\n",
      "step=5000, eval_return=121.0, samples_per_sec=1610\n",
      "step=10000, eval_return=200.0, samples_per_sec=1444\n"
     ]
    }
   ],
   "source": [
    "agent = ActorCritic(args).to(args.device)\n",
    "rewards3 = train(args, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a75d648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPH RESULTS HERE (MEAN + STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19ea6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRAPH ALL RESULTS HERE (MEAN + STD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
