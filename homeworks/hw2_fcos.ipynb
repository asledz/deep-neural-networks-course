{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VBJpAMzglRo"
   },
   "source": [
    "# Anchor-free single-stage object detection with FCOS\n",
    "\n",
    "In this exercise your goal will be to solve an object detection training and prediction task using the anchor-free single-stage approach.\n",
    "\n",
    "There are 10 points to get in total.\n",
    "\n",
    "## TLDR; overview\n",
    "\n",
    "In this task one should:\n",
    "- build an object detection model using the variant of `FCOS`,\n",
    "- train an object detection model.\n",
    "\n",
    "Hints and comments:\n",
    "\n",
    "- Model architecture and loss are heavily inspired by [FCOS](https://arxiv.org/pdf/1904.01355.pdf) paper,\n",
    "- you can freely subclass and extend the interface of classes in this exercise,\n",
    "- be sure that you understand the concept of anchor-free object detection. There are many tutorials and articles about it (e.g. [this](https://medium.com/swlh/fcos-walkthrough-the-fully-convolutional-approach-to-object-detection-777f614268c) one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PsyO2OdlLLE"
   },
   "source": [
    "### Data description\n",
    "\n",
    "In this task we will paste bounding boxes with digits **from 1 to 5** randomly selected from `MNIST` dataset on a canvas of size `(128, 128)` and **randomly scaled by a factor between 0.5 and 1.0**. We assume that:\n",
    "\n",
    "- the two boxes from a canvas should have no more than `0.1` of `iou` overlap,\n",
    "- the digits are fully contained in canvas,\n",
    "- boxes are modeled using `MnistBox` class,\n",
    "- canvas is modeled using `MnistCanvas` class.\n",
    "\n",
    "Let us have a look at definition of these classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "L1rAdIiRq2G8"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MnistBox:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_min: int,\n",
    "        y_min: int,\n",
    "        x_max: int,\n",
    "        y_max: int,\n",
    "        class_nb: Optional[int] = None,\n",
    "        rotated: Optional[bool] = None,\n",
    "    ):\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.y_min = y_min\n",
    "        self.y_max = y_max\n",
    "        self.class_nb = class_nb\n",
    "        self.rotated = rotated\n",
    "    \n",
    "    @property\n",
    "    def x_diff(self):\n",
    "        return self.x_max - self.x_min\n",
    "    \n",
    "    @property\n",
    "    def y_diff(self):\n",
    "        return self.y_max - self.y_min\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Mnist Box: x_min = {self.x_min},' +\\\n",
    "               f' x_max = {self.x_max}, y_min = {self.y_min},' +\\\n",
    "               f' y_max = {self.y_max}. Class = {self.class_nb}.' +\\\n",
    "               f' Rotated = {self.rotated}.'\n",
    "\n",
    "    def plot_on_ax(self, ax, color: Optional[str] = 'r'):\n",
    "        ax.add_patch(\n",
    "            patches.Rectangle(\n",
    "                (self.y_min, self.x_min),\n",
    "                 self.y_diff,\n",
    "                 self.x_diff,\n",
    "                 linewidth=1,\n",
    "                 edgecolor=color,\n",
    "                 facecolor='none',\n",
    "            )\n",
    "        )\n",
    "        ax.text(\n",
    "            self.y_min,\n",
    "            self.x_min,\n",
    "            f'{self.class_nb}' if not self.rotated else f'{self.class_nb}*',\n",
    "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
    "            clip_box=ax.clipbox,\n",
    "            clip_on=True,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def area(self):\n",
    "        return max((self.x_max - self.x_min), 0) * max((self.y_max - self.y_min), 0)\n",
    "\n",
    "    def iou_with(self, other_box: \"MnistBox\"):\n",
    "        aux_box = MnistBox(\n",
    "            x_min=max(self.x_min, other_box.x_min),\n",
    "            x_max=min(self.x_max, other_box.x_max),\n",
    "            y_min=max(self.y_min, other_box.y_min),\n",
    "            y_max=min(self.y_max, other_box.y_max),\n",
    "        ) \n",
    "        return aux_box.area / (self.area + other_box.area - aux_box.area)\n",
    "\n",
    "\n",
    "class MnistCanvas:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        boxes: List[MnistBox],\n",
    "    ):\n",
    "        self.image = image\n",
    "        self.boxes = boxes\n",
    "        self.shape = (1, 1, self.image.shape[0], self.image.shape[1])\n",
    "\n",
    "    def add_digit(\n",
    "        self,\n",
    "        digit: np.ndarray,\n",
    "        class_nb: int,\n",
    "        x_min: int,\n",
    "        y_min: int,\n",
    "        rotated=None,\n",
    "        iou_threshold=0.1,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Add a digit to an image if it does not overlap with existing boxes\n",
    "        above iou_threshold.\n",
    "        \"\"\"\n",
    "        image_x, image_y = digit.shape\n",
    "        if x_min >= self.image.shape[0] and y_min >= self.image.shape[1]:\n",
    "            raise ValueError('Wrong initial corner box')\n",
    "        new_box_x_min = x_min\n",
    "        new_box_y_min = y_min\n",
    "        new_box_x_max = min(x_min + image_x, self.image.shape[0])\n",
    "        new_box_y_max = min(y_min + image_y, self.image.shape[1])\n",
    "        new_box = MnistBox(\n",
    "            x_min=new_box_x_min,\n",
    "            x_max=new_box_x_max,\n",
    "            y_min=new_box_y_min,\n",
    "            y_max=new_box_y_max,\n",
    "            class_nb=class_nb,\n",
    "            rotated=rotated,\n",
    "        )\n",
    "        old_background = self.image[\n",
    "            new_box_x_min:new_box_x_max,\n",
    "            new_box_y_min:new_box_y_max\n",
    "        ]\n",
    "        for box in self.boxes:\n",
    "            if new_box.iou_with(box) > iou_threshold:\n",
    "                return False\n",
    "        self.image[\n",
    "            new_box_x_min:new_box_x_max,\n",
    "            new_box_y_min:new_box_y_max\n",
    "        ] = np.maximum(old_background, digit)\n",
    "        self.boxes.append(\n",
    "            new_box\n",
    "        ) \n",
    "        return True\n",
    "        \n",
    "    def get_torch_tensor(self) -> torch.Tensor:\n",
    "        np_image = self.image.astype('float32')\n",
    "        np_image = np_image.reshape(\n",
    "            (1, 1, self.image.shape[0], self.image.shape[1])\n",
    "        )\n",
    "        return torch.from_numpy(np_image).to(DEVICE)\n",
    "\n",
    "    @classmethod\n",
    "    def get_empty_of_size(cls, size: Tuple[int, int]):\n",
    "        return cls(\n",
    "            image=np.zeros(size),\n",
    "            boxes=[],\n",
    "        )\n",
    "\n",
    "    def plot(self, boxes: Optional[List[MnistBox]] = None):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(self.image)\n",
    "        boxes = boxes or self.boxes\n",
    "        for box in boxes:\n",
    "            box.plot_on_ax(ax)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWMxgsgFtlze"
   },
   "source": [
    "Each canvas has 3-6 boxes with randomly selected digits. The digits for training data are from first 10K examples from `MNIST` train data. The digits for test data are selected from first 1K examples from `MNIST` test data. The Dataset is generated using the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HezSZXw4z-cx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 02:05:50.482254: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 02:05:51.027085: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 02:05:51.027123: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-19 02:05:51.027128: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import skimage.transform as st\n",
    "\n",
    "\n",
    "mnist_data = mnist.load_data()\n",
    "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist_data\n",
    "\n",
    "\n",
    "def crop_insignificant_values(digit:np.ndarray, threshold=0.1):\n",
    "    bool_digit = digit > threshold\n",
    "    x_range = bool_digit.max(axis=0)\n",
    "    y_range = bool_digit.max(axis=1)\n",
    "    start_x = (x_range.cumsum() == 0).sum()\n",
    "    end_x = (x_range[::-1].cumsum() == 0).sum()\n",
    "    start_y = (y_range.cumsum() == 0).sum()\n",
    "    end_y = (y_range[::-1].cumsum() == 0).sum()\n",
    "    return digit[start_y:-end_y - 1, start_x:-end_x - 1]\n",
    "\n",
    "\n",
    "TRAIN_DIGITS = [\n",
    "    crop_insignificant_values(digit) / 255.0\n",
    "    for digit_index, digit in enumerate(mnist_x_train[:10000])\n",
    "]\n",
    "TRAIN_CLASSES = mnist_y_train[:10000]\n",
    "\n",
    "TEST_DIGITS = [\n",
    "    crop_insignificant_values(digit) / 255.0\n",
    "    for digit_index, digit in enumerate(mnist_x_test[:1000])\n",
    "]\n",
    "TEST_CLASSES = mnist_y_test[:1000]\n",
    "\n",
    "\n",
    "def get_random_canvas(\n",
    "    digits: Optional[List[np.ndarray]] = None,\n",
    "    classes: Optional[List[int]] = None,\n",
    "    nb_of_digits: Optional[int] = None,\n",
    "    labels = [0, 1, 2, 3, 4]\n",
    "    ):\n",
    "    digits = digits if digits is not None else TRAIN_DIGITS\n",
    "    classes = classes if classes is not None else TRAIN_CLASSES\n",
    "    nb_of_digits = nb_of_digits if nb_of_digits is not None else np.random.randint(low=3, high=6 + 1)\n",
    "    new_canvas = MnistCanvas.get_empty_of_size(size=(128, 128))\n",
    "    attempts_done = 0\n",
    "    while attempts_done < nb_of_digits:\n",
    "        current_digit_index = np.random.randint(len(digits))\n",
    "        current_digit_class = classes[current_digit_index]\n",
    "        if current_digit_class not in labels:\n",
    "            continue\n",
    "        rescale = np.random.random() > 0.5\n",
    "        current_digit = digits[current_digit_index]\n",
    "        if rescale:\n",
    "            factor = (np.random.random() / 2) + 0.5\n",
    "            current_digit = st.resize(\n",
    "                current_digit, \n",
    "                (int(current_digit.shape[0] * factor), int(current_digit.shape[1] * factor)))\n",
    "            # current_digit = np.rot90(current_digit)\n",
    "        random_x_min = np.random.randint(0, 128 - current_digit.shape[0] - 3)\n",
    "        random_y_min = np.random.randint(0, 128 - current_digit.shape[1] - 3)\n",
    "        if new_canvas.add_digit(\n",
    "            digit=current_digit,\n",
    "            x_min=random_x_min,\n",
    "            y_min=random_y_min,\n",
    "            class_nb=current_digit_class,\n",
    "            rotated=rescale,\n",
    "        ):\n",
    "            attempts_done += 1\n",
    "    return new_canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i2OjUEC7eaC"
   },
   "source": [
    "Let us have a look at example canvas (rescaled digits have additional *added to description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "OsLpINOtvhd8",
    "outputId": "94f797f5-d759-420e-a433-30325a160487"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist_canvas = get_random_canvas()\n",
    "mnist_canvas.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgU2bKyJ3pVh"
   },
   "source": [
    "For training one can either:\n",
    "- generate `TRAIN_CANVAS` similarly to `TEST_CANVAS` creation,\n",
    "- use the fact that `get_random_canvas()` generates a random train canvas and generate training data on-the-fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GpJZUkDGJSi"
   },
   "source": [
    "### Model building (5 pt.)\n",
    "\n",
    "\n",
    "One should build a model for digit detection in $\\texttt{pytorch}$. Model should consist of:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qdCBH9PkT2C"
   },
   "source": [
    "#### $\\texttt{backbone}$:\n",
    "\n",
    "We provided you with a backbone model architecture paired with Feature Pyramid Network (`BackboneWithFPN`) that accepts a `MnistCanvas` instance and output a dictionary, which has a FPN group name as a keys and their tensors as value.\n",
    "For a FPN with strides set to [32, 64, 128] and number of output channels set to 64, the sizes of the tensors will be [1, 64, 128, 128], [1, 64, 64, 64], [1, 64, 32, 32] consecutively. This module should be trained together with the rest of your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kXco8riNGHhl"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from torch import nn, Tensor\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
    "\n",
    "\n",
    "class Backbone(torch.nn.Module):\n",
    "    def __init__(self, strides = [8, 16, 32]):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.first_block = torch.nn.Sequential(\n",
    "            nn.Conv2d(1, strides[0], (3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList(\n",
    "            [torch.nn.Sequential(*[\n",
    "                nn.Conv2d(strides[i-1], strides[i], (3, 3), padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2, 2),\n",
    "              ]) for i in range(1, len(strides))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        image = x.to(DEVICE).view(1, 1, 128, 128)\n",
    "        x = self.first_block(image)\n",
    "        aux = [x]\n",
    "        for block in self.blocks:\n",
    "            x = block(aux[-1])\n",
    "            aux.append(x)\n",
    "        return aux\n",
    "\n",
    "\n",
    "class BackboneWithFPN(torch.nn.Module):\n",
    "    def __init__(self, strides, out_channels=32) -> None:\n",
    "        super().__init__()\n",
    "        self.strides = strides\n",
    "        self.out_channels = out_channels\n",
    "        self.backbone = Backbone(self.strides)\n",
    "        self.fpn = FeaturePyramidNetwork(self.strides, self.out_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        output_backbone = self.backbone(x)\n",
    "        \n",
    "        x = OrderedDict()\n",
    "        for i, f in enumerate(output_backbone):\n",
    "            x[f'feat{i}'] = f\n",
    "        output_fpn = self.fpn(x)\n",
    "        return output_fpn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkOLl12nkiI8"
   },
   "source": [
    "#### $\\texttt{anchor generator}$:\n",
    "\n",
    "FCOS is anchor-free in a typical sense of this word, but it can also be said that there is one pixel-wise \"anchor\" per localisation on a given feature map.\n",
    "Therefore, anchor generator from `torchvision` is used for convenience.\n",
    "You will obtain $128^2 + 64^2 + 32^2 = 21504$ locations in total for the previously chosen strides.\n",
    "They will be called anchors in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dyd2-zOxf9VM",
    "outputId": "b7f7eea6-d87e-40e5-e3e5-05ca2d3e8fd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 1, 1]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example code - anchor generator is already included in the code later\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "\n",
    "anchor_sizes = ((32,), (64,), (128,))  # equal to strides of FPN multi-level feature map\n",
    "aspect_ratios = ((1.0,),) * len(anchor_sizes)  # set only one anchor for each level\n",
    "anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n",
    "anchor_generator.num_anchors_per_location()\n",
    "# notice that effectively one anchor is one location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iM3oSesif-F5"
   },
   "outputs": [],
   "source": [
    "# Later in the code you will use the anchor generator in the following way:\n",
    "# anchors = anchor_generator(images, features)\n",
    "# [x.size(2) * x.size(3) for x in features] # recover level sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTYyCuMdmBk7"
   },
   "source": [
    "#### $\\texttt{FCOSClassificationHead}$ (1 pt.):\n",
    "\n",
    "Write a classification head to be used in FCOS.\n",
    "The input is is the output of `BackboneWithFPN` forward call.\n",
    "This module should contain $n$ blocks with `nn.Conv2d`, `nn.GroupNorm`, and `nn.ReLU` each (in the paper, $n=4$).\n",
    "Each convolutional layer should input and output `self.out_channels` channels.\n",
    "The additional final block should be `nn.Conv2d` outputting `C` channels.\n",
    "The final output should consist of classification logits of shape `(N, A, C)`, where `N` means the number of samples in a batch, `A` is the sum of all FPN strides (21504 in the aforementioned case), and `C` is the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kpDP9QVjnn05"
   },
   "outputs": [],
   "source": [
    "CLASS_HEAD_KERNEL_SIZE = 3\n",
    "CLASS_HEAD_NUM_GROUPS = 8\n",
    "\n",
    "class FCOSClassificationHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        num_classes: int,\n",
    "        num_convs: int = 4,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: your code here\n",
    "        ################################################################################################\n",
    "\n",
    "        # Helped myself with: https://pytorch.org/vision/main/_modules/torchvision/models/detection/fcos.html\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.num_convs = num_convs\n",
    "\n",
    "        self.blocks = torch.nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, self.in_channels, CLASS_HEAD_KERNEL_SIZE),\n",
    "                nn.GroupNorm(self.in_channels, self.in_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            for _ in range(self.num_convs)\n",
    "        ])\n",
    "\n",
    "        self.last_block = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, 1 * num_classes, CLASS_HEAD_KERNEL_SIZE) # ?????????\n",
    "        )\n",
    "        ################################################################################################\n",
    "        # end of your code\n",
    "\n",
    "    def forward(self, x: List[Tensor]) -> Tensor:\n",
    "        # TODO: your code here\n",
    "        ################################################################################################\n",
    "        mylist = []\n",
    "\n",
    "        for feature in x:\n",
    "            myx = feature\n",
    "            for block in self.blocks:\n",
    "                myx = block(myx)\n",
    "            myx = self.last_block(myx)\n",
    "\n",
    "            N, _, H, W = myx.shape\n",
    "            myx = myx.view(N, -1, self.num_classes, H, W)\n",
    "            myx = myx.permute(0, 3, 4, 1, 2)\n",
    "            myx = myx.reshape(N, -1, self.num_classes)\n",
    "            mylist.append(myx)\n",
    "\n",
    "        return torch.cat(mylist, dim=1)\n",
    "        \n",
    "        ################################################################################################\n",
    "        # end of your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcHAnQy2mFJU"
   },
   "source": [
    "#### $\\texttt{FCOSRegressionHead}$  (1 pt.):\n",
    "\n",
    "Write a regression head to be used in FCOS - both for bounding boxes and center-ness.\n",
    "The input is the output of `BackboneWithFPN` forward call.\n",
    "This module should contain $n$ blocks with `nn.Conv2d`, `nn.GroupNorm`, and `nn.ReLU` each (in the paper, $n=4$), which will be shared for regression and center-ness.\n",
    "The final block for bounding box regression should be `nn.Conv2d` and have `4` channels and it should be followed by relu functional to get rid of negative values.\n",
    "The final block for center-ness regression should be `nn.Conv2d` and have `1` channel.\n",
    "The output should consist of a tuple of tensors (bounding box regression and center-ness).\n",
    "Bounding box regression logits should be of shape `(N, A, 4)`, whereas for center-ness that would be `(N, A, 1)`.\n",
    "Similarly, `N` means the number of samples in a batch, `A` is the sum of all FPN strides (21504 in the aforementioned case), and `C` is the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3H6KNzi1mEhg"
   },
   "outputs": [],
   "source": [
    "REGRESSION_HEAD_KERNEL_SIZE = (3, 3)\n",
    "REG_HEAD_NUM_ANCHORS = 1\n",
    "REG_HEAD_NUM_GROUPS = 8\n",
    "\n",
    "class FCOSRegressionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        num_convs: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: your code here\n",
    "        ################################################################################################\n",
    "        # Helped myself with: https://pytorch.org/vision/main/_modules/torchvision/models/detection/fcos.html\n",
    "        \n",
    "        self.norm_layer = partial(nn.GroupNorm, 32)\n",
    "        blocks = []\n",
    "        for _ in range(num_convs):\n",
    "          blocks.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1))\n",
    "          blocks.append(nn.GroupNorm(REG_HEAD_NUM_GROUPS, in_channels))\n",
    "          blocks.append(nn.ReLU())\n",
    "\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "        self.bbox_reg = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, REG_HEAD_NUM_ANCHORS * 4, kernel_size = REGRESSION_HEAD_KERNEL_SIZE, stride = 1, padding = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.bbox_ctrness = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, REG_HEAD_NUM_ANCHORS * 1, kernel_size = REGRESSION_HEAD_KERNEL_SIZE, stride = 1, padding = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # end of your code\n",
    "        ################################################################################################\n",
    "        \n",
    "\n",
    "    def forward(self, x: List[Tensor]) -> Tuple[Tensor, Tensor]:\n",
    "        # TODO: your code here\n",
    "        ################################################################################################\n",
    "\n",
    "        regbox_list = []\n",
    "        ctrbox_list = []\n",
    "\n",
    "        for feature in x:\n",
    "            myx = feature\n",
    "            myx = self.blocks(myx)\n",
    "\n",
    "            ## Regression\n",
    "            bbox_regression = self.bbox_reg(myx)\n",
    "            N, _, H, W = bbox_regression.shape\n",
    "            regbox = bbox_regression.view(N, -1, 4, H, W)\n",
    "            regbox = regbox.permute(0, 3, 4, 1, 2)\n",
    "            regbox = regbox.reshape(N, -1, 4)\n",
    "            regbox_list.append(regbox)\n",
    "\n",
    "            # Centerness\n",
    "            bbox_ctrness = self.bbox_ctrness(myx)\n",
    "            N, _, H, W = bbox_ctrness.shape\n",
    "            ctrbox = bbox_ctrness.view(N, -1, 1, H, W)\n",
    "            ctrbox = ctrbox.permute(0, 3, 4, 1, 2)\n",
    "            ctrbox = ctrbox.reshape(N, -1, 1)\n",
    "            ctrbox_list.append(ctrbox)\n",
    "\n",
    "        reg_result = torch.cat(regbox_list, dim=1)\n",
    "        ctr_result = torch.cat(ctrbox_list, dim=1)\n",
    "\n",
    "        return reg_result, ctr_result\n",
    "        ################################################################################################\n",
    "        # end of your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds0I47ydkvUL"
   },
   "source": [
    "#### $\\texttt{FCOSHead}$ (2 pt.):\n",
    "\n",
    "Here, the computation of the foreground indices and losses takes place.\n",
    "\n",
    "##### Loss calculation\n",
    "Compute the losses. \n",
    "They should be calculated on the positive locations/anchors, so use the foreground mask from the previous excercise.\n",
    "For regression, use `self.box_coder.decode_single` and `self.box_coder.decode_single` to move between standard (x, y, x, y) and FCOS (l, t, r, b) bounding box format.\n",
    "There are three losses to be written.\n",
    "- classification loss (with `torchvision.ops.sigmoid_focal_loss`). (1 pt.)\n",
    "- Bounding box regression (with `torchvision.ops.generalized_box_iou_loss`). Decode predictions with `self.box_coder.decode_single` before regressing against the ground truth. (1 pt.)\n",
    "- ctrness loss (`torchvision.ops.sigmoid_focal_loss`). Use Equation 3 from the paper to calculate the grond truth for the center-ness. (2 pt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PX8P0s-jOK-F"
   },
   "outputs": [],
   "source": [
    "class BoxLinearCoder:\n",
    "    \"\"\"\n",
    "    The linear box-to-box transform defined in FCOS. The transformation is parameterized\n",
    "    by the distance from the center of (square) src box to 4 edges of the target box.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalize_by_size: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            normalize_by_size (bool): normalize deltas by the size of src (anchor) boxes.\n",
    "        \"\"\"\n",
    "        self.normalize_by_size = normalize_by_size\n",
    "\n",
    "    def encode_single(self, reference_boxes: Tensor, proposals: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Encode a set of proposals with respect to some reference boxes\n",
    "\n",
    "        Args:\n",
    "            reference_boxes (Tensor): reference boxes\n",
    "            proposals (Tensor): boxes to be encoded\n",
    "\n",
    "        Returns:\n",
    "            Tensor: the encoded relative box offsets that can be used to\n",
    "            decode the boxes.\n",
    "        \"\"\"\n",
    "        # get the center of reference_boxes\n",
    "        reference_boxes_ctr_x = 0.5 * (reference_boxes[:, 0] + reference_boxes[:, 2])\n",
    "        reference_boxes_ctr_y = 0.5 * (reference_boxes[:, 1] + reference_boxes[:, 3])\n",
    "\n",
    "        # get box regression transformation deltas\n",
    "        target_l = reference_boxes_ctr_x - proposals[:, 0]\n",
    "        target_t = reference_boxes_ctr_y - proposals[:, 1]\n",
    "        target_r = proposals[:, 2] - reference_boxes_ctr_x\n",
    "        target_b = proposals[:, 3] - reference_boxes_ctr_y\n",
    "\n",
    "        targets = torch.stack((target_l, target_t, target_r, target_b), dim=1)\n",
    "        if self.normalize_by_size:\n",
    "            reference_boxes_w = reference_boxes[:, 2] - reference_boxes[:, 0]\n",
    "            reference_boxes_h = reference_boxes[:, 3] - reference_boxes[:, 1]\n",
    "            reference_boxes_size = torch.stack(\n",
    "                (reference_boxes_w, reference_boxes_h, reference_boxes_w, reference_boxes_h), dim=1\n",
    "            )\n",
    "            targets = targets / reference_boxes_size\n",
    "\n",
    "        return targets\n",
    "\n",
    "    def decode_single(self, rel_codes: Tensor, boxes: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        From a set of original boxes and encoded relative box offsets,\n",
    "        get the decoded boxes.\n",
    "\n",
    "        Args:\n",
    "            rel_codes (Tensor): encoded boxes\n",
    "            boxes (Tensor): reference boxes.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: the predicted boxes with the encoded relative box offsets.\n",
    "        \"\"\"\n",
    "\n",
    "        boxes = boxes.to(rel_codes.dtype)\n",
    "\n",
    "        ctr_x = 0.5 * (boxes[:, 0] + boxes[:, 2])\n",
    "        ctr_y = 0.5 * (boxes[:, 1] + boxes[:, 3])\n",
    "        if self.normalize_by_size:\n",
    "            boxes_w = boxes[:, 2] - boxes[:, 0]\n",
    "            boxes_h = boxes[:, 3] - boxes[:, 1]\n",
    "            boxes_size = torch.stack((boxes_w, boxes_h, boxes_w, boxes_h), dim=1)\n",
    "            rel_codes = rel_codes * boxes_size\n",
    "\n",
    "        pred_boxes1 = ctr_x - rel_codes[:, 0]\n",
    "        pred_boxes2 = ctr_y - rel_codes[:, 1]\n",
    "        pred_boxes3 = ctr_x + rel_codes[:, 2]\n",
    "        pred_boxes4 = ctr_y + rel_codes[:, 3]\n",
    "        pred_boxes = torch.stack((pred_boxes1, pred_boxes2, pred_boxes3, pred_boxes4), dim=1)\n",
    "        return pred_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nDX5c0lw9ZxM"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torchvision.ops import sigmoid_focal_loss, generalized_box_iou_loss\n",
    "from torchvision.ops import boxes as box_ops\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "\n",
    "class FCOSHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A regression and classification head for use in FCOS.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of channels of the input feature\n",
    "        num_classes (int): number of classes to be predicted\n",
    "        num_convs (Optional[int]): number of conv layer of head. Default: 4.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, num_classes: int, num_convs: Optional[int] = 4) -> None:\n",
    "        super().__init__()\n",
    "        self.box_coder = BoxLinearCoder(normalize_by_size=True)\n",
    "        self.classification_head = FCOSClassificationHead(in_channels, num_classes, num_convs)\n",
    "        self.regression_head = FCOSRegressionHead(in_channels, num_convs)\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        targets: List[Dict[str, Tensor]],\n",
    "        head_outputs: Dict[str, Tensor],\n",
    "        anchors: List[Tensor],             # anchors/locations\n",
    "        matched_idxs: List[Tensor],        # tells to which bounding box anchors are matched, -1 mean no matches\n",
    "    ) -> Dict[str, Tensor]:\n",
    "\n",
    "        cls_logits = head_outputs[\"cls_logits\"]  # [N, A, C]\n",
    "        bbox_regression = head_outputs[\"bbox_regression\"]  # [N, A, 4]\n",
    "        bbox_ctrness = head_outputs[\"bbox_ctrness\"]  # [N, A, 1]\n",
    "\n",
    "        all_gt_classes_targets = []\n",
    "        all_gt_boxes_targets = []\n",
    "        \n",
    "        for targets_per_image, matched_idxs_per_image in zip(targets, matched_idxs):\n",
    "            gt_classes_targets = targets_per_image[\"labels\"][matched_idxs_per_image.clip(min=0)]\n",
    "            gt_boxes_targets = targets_per_image[\"boxes\"][matched_idxs_per_image.clip(min=0)]\n",
    "            gt_classes_targets[matched_idxs_per_image < 0] = -1  # background\n",
    "            all_gt_classes_targets.append(gt_classes_targets)\n",
    "            all_gt_boxes_targets.append(gt_boxes_targets)\n",
    "\n",
    "        all_gt_classes_targets = torch.stack(all_gt_classes_targets)\n",
    "        print(all_gt_classes_targets.shape)\n",
    "\n",
    "        foregroud_mask = all_gt_classes_targets >= 0        \n",
    "        num_foreground = foregroud_mask.sum().item()\n",
    "        \n",
    "        loss_cls = self.compute_loss_cls(cls_logits, all_gt_classes_targets, foregroud_mask)\n",
    "        loss_bbox_reg = self.compute_loss_bbox_reg(anchors, bbox_regression, all_gt_boxes_targets, foregroud_mask)\n",
    "        loss_bbox_ctrness = self.compute_loss_ctrness(anchors, bbox_ctrness, all_gt_boxes_targets, foregroud_mask)\n",
    "\n",
    "        return {\n",
    "            \"classification\": loss_cls / max(1, num_foreground),\n",
    "            \"bbox_regression\": loss_bbox_reg / max(1, num_foreground),\n",
    "            \"bbox_ctrness\": loss_bbox_ctrness / max(1, num_foreground),\n",
    "        }\n",
    "\n",
    "    def compute_loss_ctrness(self, anchors, bbox_ctrness, all_gt_boxes_targets, foregroud_mask):\n",
    "        # pass # TODO: your code here \n",
    "        # ctrness loss\n",
    "        pass\n",
    "        ################################################################################################ \n",
    "        ## ctrness loss (torchvision.ops.sigmoid_focal_loss). Use Equation 3 from the paper to calculate the grond truth for the center-ness. (2 pt.)\n",
    "\n",
    "        \n",
    "        ################################################################################################\n",
    "\n",
    "    def compute_loss_bbox_reg(self, anchors, bbox_regression, all_gt_boxes_targets, foregroud_mask):\n",
    "        pass # TODO: your code here \n",
    "        # regression loss: GIoU loss\n",
    "        ################################################################################################\n",
    "        \n",
    "        \n",
    "        ################################################################################################\n",
    "\n",
    "    def compute_loss_cls(self, cls_logits, all_gt_classes_targets, foregroud_mask):\n",
    "        pass # TODO: your code here \n",
    "        # classification loss\n",
    "        ################################################################################################\n",
    "        \n",
    "        \n",
    "        ################################################################################################\n",
    "\n",
    "    def forward(self, x: List[Tensor]) -> Dict[str, Tensor]:\n",
    "        cls_logits = self.classification_head(x)\n",
    "        bbox_regression, bbox_ctrness = self.regression_head(x)\n",
    "        return {\n",
    "            \"cls_logits\": cls_logits,\n",
    "            \"bbox_regression\": bbox_regression,\n",
    "            \"bbox_ctrness\": bbox_ctrness,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFYNzC9KOK-G"
   },
   "source": [
    "#### Post-processing (1 pt.)\n",
    "Fill the gaps in the postprocessing routine.\n",
    "The paper states: \n",
    "\n",
    "> (...) the final score (used for ranking the detected bounding boxes) \n",
    "> is computed by multiplying the predicted center-ness with the corresponding classification score. \n",
    "> Thus the center-ness can downweight the scores of bounding boxes far from the center of an object. \n",
    "> As a result, with high probability, these low-quality bounding boxes might be filtered out by \n",
    "> the final non-maximum suppression (NMS) process, improving the detection performance remarkably.\n",
    "\n",
    "1. Remove boxes with score smaller than `self.score_thresh`. The score is given by `sqrt`($\\sigma$(`classification_score`) * $\\sigma$(`cente-ness_score`)) (1pt.)\n",
    "2. Keep only top `self.topk_candidates` scoring predictions (1pt.)\n",
    "\n",
    "The `compute_loss` function here calculates the indexes of matched classes for each anchor/location for your convenience in later calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EB17o3zs1JiR"
   },
   "outputs": [],
   "source": [
    "class FCOS(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        num_classes: int,\n",
    "        # transform parameters\n",
    "        image_mean: Optional[List[float]] = None,\n",
    "        image_std: Optional[List[float]] = None,\n",
    "        # Anchor parameters\n",
    "        anchor_generator: AnchorGenerator = None,\n",
    "        center_sampling_radius: float = 1.5,\n",
    "        score_thresh: float = 0.2,\n",
    "        nms_thresh: float = 0.6,\n",
    "        detections_per_img: int = 100,\n",
    "        topk_candidates: int = 1000,\n",
    "        num_convs_in_heads:int = 4,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.anchor_generator = anchor_generator\n",
    "        self.head = FCOSHead(backbone.out_channels, num_classes, num_convs=num_convs_in_heads)\n",
    "        self.box_coder = BoxLinearCoder(normalize_by_size=True)\n",
    "        self.transform = GeneralizedRCNNTransform(128, 128, image_mean, image_std, **kwargs)\n",
    "\n",
    "        self.center_sampling_radius = center_sampling_radius\n",
    "        self.score_thresh = score_thresh\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.detections_per_img = detections_per_img\n",
    "        self.topk_candidates = topk_candidates\n",
    "\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        targets: List[Dict[str, Tensor]],\n",
    "        head_outputs: Dict[str, Tensor],\n",
    "        anchors: List[Tensor],\n",
    "        num_anchors_per_level: List[int],\n",
    "    ) -> Dict[str, Tensor]:\n",
    "        matched_idxs = []\n",
    "        for anchors_per_image, targets_per_image in zip(anchors, targets): # batch\n",
    "            if targets_per_image[\"boxes\"].numel() == 0:\n",
    "                matched_idxs.append(\n",
    "                    torch.full((anchors_per_image.size(0),), -1, dtype=torch.int64, device=anchors_per_image.device)\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            gt_boxes = targets_per_image[\"boxes\"]\n",
    "            gt_centers = (gt_boxes[:, :2] + gt_boxes[:, 2:]) / 2  # Nx2                 # Calculate centres of bounding boxes\n",
    "            anchor_centers = (anchors_per_image[:, :2] + anchors_per_image[:, 2:]) / 2  # N  \n",
    "            anchor_sizes = anchors_per_image[:, 2] - anchors_per_image[:, 0]            # Match anchors\n",
    "            # center sampling: anchor point must be close enough to gt center.\n",
    "            pairwise_match = (anchor_centers[:, None, :] - gt_centers[None, :, :]).abs_().max(\n",
    "                dim=2\n",
    "            ).values < self.center_sampling_radius * anchor_sizes[:, None]\n",
    "            # compute pairwise distance between N points and M boxes\n",
    "            x, y = anchor_centers.unsqueeze(dim=2).unbind(dim=1)  # (N, 1)\n",
    "            x0, y0, x1, y1 = gt_boxes.unsqueeze(dim=0).unbind(dim=2)  # (1, M)\n",
    "            pairwise_dist = torch.stack([x - x0, y - y0, x1 - x, y1 - y], dim=2)  # (N, M)\n",
    "\n",
    "            # anchor point must be inside gt\n",
    "            pairwise_match &= pairwise_dist.min(dim=2).values > 0\n",
    "\n",
    "            # each anchor is only responsible for certain scale range.\n",
    "            lower_bound = anchor_sizes * 4\n",
    "            lower_bound[: num_anchors_per_level[0]] = 0\n",
    "            upper_bound = anchor_sizes * 8\n",
    "            upper_bound[-num_anchors_per_level[-1] :] = float(\"inf\")\n",
    "            pairwise_dist = pairwise_dist.max(dim=2).values\n",
    "            pairwise_match &= (pairwise_dist > lower_bound[:, None]) & (pairwise_dist < upper_bound[:, None])\n",
    "\n",
    "            # match the GT box with minimum area, if there are multiple GT matches\n",
    "            gt_areas = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])  # N\n",
    "            pairwise_match = pairwise_match.to(torch.float32) * (1e8 - gt_areas[None, :])\n",
    "            min_values, matched_idx = pairwise_match.max(dim=1)  # R, per-anchor match\n",
    "            matched_idx[min_values < 1e-5] = -1  # unmatched anchors are assigned -1\n",
    "\n",
    "            matched_idxs.append(matched_idx)\n",
    "        # end of your code\n",
    "        # matched index - anchor-to-target match\n",
    "        return self.head.compute_loss(targets, head_outputs, anchors, matched_idxs)\n",
    "\n",
    "    def postprocess_detections(\n",
    "        self, head_outputs: Dict[str, List[Tensor]], anchors: List[List[Tensor]], image_shapes: List[Tuple[int, int]]\n",
    "    ) -> List[Dict[str, Tensor]]:\n",
    "        class_logits = head_outputs[\"cls_logits\"]\n",
    "        box_regression = head_outputs[\"bbox_regression\"]\n",
    "        box_ctrness = head_outputs[\"bbox_ctrness\"]\n",
    "\n",
    "        pass # TODO: your code here\n",
    "        num_images = len(image_shapes)\n",
    "\n",
    "        detections: List[Dict[str, Tensor]] = []\n",
    "\n",
    "        for index in range(num_images):\n",
    "            box_regression_per_image = [br[index] for br in box_regression]\n",
    "            logits_per_image = [cl[index] for cl in class_logits]\n",
    "            box_ctrness_per_image = [bc[index] for bc in box_ctrness]\n",
    "            anchors_per_image, image_shape = anchors[index], image_shapes[index]\n",
    "\n",
    "            image_boxes = []\n",
    "            image_scores = []\n",
    "            image_labels = []\n",
    "\n",
    "            for box_regression_per_level, logits_per_level, box_ctrness_per_level, anchors_per_level in zip(\n",
    "                box_regression_per_image, logits_per_image, box_ctrness_per_image, anchors_per_image\n",
    "            ):\n",
    "                num_classes = logits_per_level.shape[-1]\n",
    "                \n",
    "                pass # TODO: your code here\n",
    "                # Remove low scoring boxes and keep only topk scoring predictions\n",
    "                ################################################################################################\n",
    "\n",
    "                # scores_per_level = ...\n",
    "                # topk_idxs = ...\n",
    "\n",
    "                ################################################################################################\n",
    "                # end of your code\n",
    "                anchor_idxs = torch.div(topk_idxs, num_classes, rounding_mode=\"floor\")\n",
    "                labels_per_level = topk_idxs % num_classes\n",
    "\n",
    "                boxes_per_level = self.box_coder.decode_single(\n",
    "                    box_regression_per_level[anchor_idxs], anchors_per_level[anchor_idxs]\n",
    "                )\n",
    "                boxes_per_level = box_ops.clip_boxes_to_image(boxes_per_level, image_shape)\n",
    "\n",
    "                image_boxes.append(boxes_per_level)\n",
    "                image_scores.append(scores_per_level)\n",
    "                image_labels.append(labels_per_level)\n",
    "\n",
    "            image_boxes = torch.cat(image_boxes, dim=0)\n",
    "            image_scores = torch.cat(image_scores, dim=0)\n",
    "            image_labels = torch.cat(image_labels, dim=0)\n",
    "\n",
    "            # non-maximum suppression\n",
    "            keep = box_ops.batched_nms(image_boxes, image_scores, image_labels, self.nms_thresh)\n",
    "            keep = keep[: self.detections_per_img]\n",
    "\n",
    "            detections.append(\n",
    "                {\n",
    "                    \"boxes\": image_boxes[keep],\n",
    "                    \"scores\": image_scores[keep],\n",
    "                    \"labels\": image_labels[keep],\n",
    "                }\n",
    "            )\n",
    "        return detections\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images: List[Tensor],\n",
    "        targets: Optional[List[Dict[str, Tensor]]] = None,\n",
    "    ) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list[Tensor]): images to be processed\n",
    "            targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional)\n",
    "\n",
    "        Returns:\n",
    "            result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "                During training, it returns a dict[Tensor] which contains the losses.\n",
    "                During testing, it returns list[BoxList] contains additional fields\n",
    "                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "        \"\"\"\n",
    "        \n",
    "        # transform the input (normalise with std and )\n",
    "        images, targets = self.transform(images, targets)\n",
    "        print(\"Targets in transform: \", targets[0])\n",
    "        # get the features from the backbone\n",
    "        features = self.backbone(images.tensors)\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            features = OrderedDict([(\"0\", features)])\n",
    "        features = list(features.values())\n",
    "\n",
    "        # compute the fcos heads outputs using the features\n",
    "        head_outputs = self.head(features)\n",
    "\n",
    "        # create the set of anchors\n",
    "        anchors = self.anchor_generator(images, features)\n",
    "        # recover level sizes\n",
    "        num_anchors_per_level = [x.size(2) * x.size(3) for x in features]\n",
    "        \n",
    "        losses = {}\n",
    "        detections: List[Dict[str, Tensor]] = []\n",
    "        if self.training:\n",
    "            losses = self.compute_loss(targets, head_outputs, anchors, num_anchors_per_level)\n",
    "            return losses\n",
    "        else:\n",
    "            # split outputs per level\n",
    "            split_head_outputs: Dict[str, List[Tensor]] = {}\n",
    "            for k in head_outputs:\n",
    "                split_head_outputs[k] = list(head_outputs[k].split(num_anchors_per_level, dim=1))\n",
    "            split_anchors = [list(a.split(num_anchors_per_level)) for a in anchors]\n",
    "\n",
    "            # compute the detections\n",
    "            detections = self.postprocess_detections(split_head_outputs, split_anchors, images.image_sizes)\n",
    "            return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA1Nvz6jagyP"
   },
   "source": [
    "### Metrics and evaluation (2 pt.)\n",
    "\n",
    "#### Digit Accuracy (1 pt.)\n",
    "\n",
    "This method shoud accept `canvas: MnistCanvas` and `predicted_boxes: List[MnistBox]` obtained using `TargetDecoder.get_predictions` method and output whether there is a direct matching between boxes from `MnistCanvas` and predictions. There is a direct matching if:\n",
    "\n",
    "- for all boxes from `canvas`, there exist precisely one box from `predicted_boxes` with a matching class, rotation and `iou` overlap greater than `0.5`,\n",
    "- the number of `canvas` boxes match `len(predicted_boxes)`.\n",
    "\n",
    "The method shoud output `1` if there is a matching and `0` otherwise.\n",
    "\n",
    "#### Evaluation function (1 pt.)\n",
    "\n",
    "Write an evaluation function for your model.\n",
    "If needed, perform the final NMS with `torchvision.ops.nms` (threshold at `iou`) and remove redundant boxes with scores smaller than `min_score`.\n",
    "Then, calculate the average `DigitAccuracy` for each.\n",
    "You may experiment with parameters for this method, but the default ones are fine (this is not subject of our grading).\n",
    "If you are fine what you achiveded with `postprocess_detections`, you may focus solely on evaluation (although playing with this second stage NMS and score thresholding might be useful for diagnostics).\n",
    "\n",
    "The output of the method is the average digit accuracy on the test set. \n",
    "Use it to track your model performance over epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bfRZnbAP-XGG"
   },
   "outputs": [],
   "source": [
    "class DigitAccuracy:\n",
    "    def compute_metric  (\n",
    "        self,\n",
    "        predicted_boxes: List[MnistBox],\n",
    "        canvas: MnistCanvas,\n",
    "        iou: float = 0.5,\n",
    "    ):\n",
    "        # TODO: your code here\n",
    "        ################################################################################################\n",
    "        pass\n",
    "        \n",
    "        ################################################################################################\n",
    "        # end of your code\n",
    "\n",
    "def evaluate(model: nn.Module, TEST_CANVAS, min_score: float = .2, iou: float = .05) -> float:\n",
    "    # TODO: your code here\n",
    "    ################################################################################################\n",
    "    pass\n",
    "\n",
    "    ################################################################################################\n",
    "    # end of your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15n5w-hhvRbS"
   },
   "source": [
    "### Train your model (3pt)\n",
    "\n",
    "One should use all classes defined above to train the model.\n",
    "\n",
    "- Train the model. A passing threshold is `10%` of a `DigitAccuracy` on a `TEST_CANVAS` data (2 pt.).\n",
    "- Plot example results of matched and mismatched predictions (0.5 pt.).\n",
    "- Plot particular losses and evaluation score per (0.5 pt.).\n",
    "\n",
    "**Hint:** Training can take a while to achieve the expected accuracy. It is normal that for many epochs at the beginning accuracy is constantly $0$. Do not worry as long as the loss is on average decreasing across epochs. You may want to reduce number of digit classes (for example only to generate `0`s on canvas) to test the convergence (the hyperparameters might change, though!). A model with around 500k parameters should be able to hit 10% of the metric in 20 minutes (tested on a 2021 MacBook on CPU). On Google Colab with GPU it will be matter of 2 minutes, but notice that the free GPU is limited.\n",
    "\n",
    "**Even more important hint:** Pay attention to the ordering of the X/Y values of `get_torch_tensor` and align it with your target!\n",
    "\n",
    "Good luck and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eu9xbkaY1Jih",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "455d0561-0b3e-4c2d-bd3f-068699acb1ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets in transform:  {'boxes': tensor([[102.,  82., 117.,  94.],\n",
      "        [ 99., 103., 118., 106.],\n",
      "        [ 87., 107.,  97., 109.],\n",
      "        [ 20.,  32.,  39.,  36.],\n",
      "        [ 88.,  48.,  97.,  55.]], device='cuda:0'), 'labels': tensor([3., 1., 1., 1., 0.], device='cuda:0')}\n",
      "torch.Size([1, 21504])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [13], line 83\u001B[0m\n\u001B[1;32m     76\u001B[0m     label_list\u001B[38;5;241m.\u001B[39mappend(box\u001B[38;5;241m.\u001B[39mclass_nb)\n\u001B[1;32m     78\u001B[0m targets \u001B[38;5;241m=\u001B[39m [{\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mboxes\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mTensor(box_list)\u001B[38;5;241m.\u001B[39mto(DEVICE),\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mTensor(label_list)\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[1;32m     81\u001B[0m }]\n\u001B[0;32m---> 83\u001B[0m \u001B[43mfcos\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor_image\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/deep-neural-networks-course/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn [11], line 188\u001B[0m, in \u001B[0;36mFCOS.forward\u001B[0;34m(self, images, targets)\u001B[0m\n\u001B[1;32m    186\u001B[0m detections: List[Dict[\u001B[38;5;28mstr\u001B[39m, Tensor]] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[0;32m--> 188\u001B[0m     losses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhead_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43manchors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_anchors_per_level\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m losses\n\u001B[1;32m    190\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;66;03m# split outputs per level\u001B[39;00m\n",
      "Cell \u001B[0;32mIn [11], line 83\u001B[0m, in \u001B[0;36mFCOS.compute_loss\u001B[0;34m(self, targets, head_outputs, anchors, num_anchors_per_level)\u001B[0m\n\u001B[1;32m     80\u001B[0m     matched_idxs\u001B[38;5;241m.\u001B[39mappend(matched_idx)\n\u001B[1;32m     81\u001B[0m \u001B[38;5;66;03m# end of your code\u001B[39;00m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;66;03m# matched index - anchor-to-target match\u001B[39;00m\n\u001B[0;32m---> 83\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhead\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhead_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43manchors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmatched_idxs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn [10], line 60\u001B[0m, in \u001B[0;36mFCOSHead.compute_loss\u001B[0;34m(self, targets, head_outputs, anchors, matched_idxs)\u001B[0m\n\u001B[1;32m     56\u001B[0m loss_bbox_reg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_bbox_reg(anchors, bbox_regression, all_gt_boxes_targets, foregroud_mask)\n\u001B[1;32m     57\u001B[0m loss_bbox_ctrness \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_ctrness(anchors, bbox_ctrness, all_gt_boxes_targets, foregroud_mask)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m---> 60\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclassification\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mloss_cls\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_foreground\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbbox_regression\u001B[39m\u001B[38;5;124m\"\u001B[39m: loss_bbox_reg \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m1\u001B[39m, num_foreground),\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbbox_ctrness\u001B[39m\u001B[38;5;124m\"\u001B[39m: loss_bbox_ctrness \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;241m1\u001B[39m, num_foreground),\n\u001B[1;32m     63\u001B[0m }\n",
      "\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for /: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "\n",
    "TEST_SEED = 42 # DO NOT CHANGE THIS LINE.\n",
    "np.random.seed(TEST_SEED)\n",
    "\n",
    "N_TRAINING_EXAMPLES = 500\n",
    "LR = 0.0004 # for SGD with momentum=0.9\n",
    "EPOCHS = 16\n",
    "STRIDES = [32, 64, 128]\n",
    "CONVS_IN_HEADS = 4\n",
    "OUT_CHANNELS = 64\n",
    "LABELS = list(range(5))  # lower it for quick convergence testing\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "TRAIN_CANVAS = [\n",
    "    get_random_canvas(\n",
    "        digits=TEST_DIGITS,\n",
    "        classes=TEST_CLASSES,\n",
    "        labels=LABELS,\n",
    "    )\n",
    "    for _ in range(N_TRAINING_EXAMPLES)\n",
    "]\n",
    "\n",
    "TEST_CANVAS_SIZE = 256\n",
    "\n",
    "\n",
    "TEST_CANVAS = [\n",
    "    get_random_canvas(\n",
    "        digits=TEST_DIGITS,\n",
    "        classes=TEST_CLASSES,\n",
    "        labels=LABELS,\n",
    "    )\n",
    "    for _ in range(TEST_CANVAS_SIZE)\n",
    "]\n",
    "\n",
    "anchor_sizes = tuple([(x,) for x in STRIDES])  # equal to strides of multi-level feature map\n",
    "aspect_ratios = ((1.0,),) * len(anchor_sizes)  # set only one \"anchor\" per location\n",
    "anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n",
    "\n",
    "fcos = FCOS(\n",
    "    backbone = BackboneWithFPN(strides=STRIDES, out_channels=OUT_CHANNELS), \n",
    "    num_classes = len(LABELS), \n",
    "    image_mean = [0.0233],\n",
    "    image_std = [0.14],\n",
    "    num_convs_in_heads = CONVS_IN_HEADS, \n",
    "    anchor_generator = anchor_generator,\n",
    "    detections_per_img = 100\n",
    "    )\n",
    "fcos.to(DEVICE)\n",
    "\n",
    "# TODO: write your code here\n",
    "################################################################################################\n",
    "\n",
    "train_tensors_images = [canva.get_torch_tensor() for canva in TRAIN_CANVAS]\n",
    "tens = train_tensors_images[0]\n",
    "# print(tens.shape)\n",
    "\n",
    "# train_tensors_images = tr\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    for image in TRAIN_CANVAS:\n",
    "        # print(image)\n",
    "        tensor_image = image.get_torch_tensor()\n",
    "\n",
    "        box_list = []\n",
    "        label_list = []\n",
    "        for box in image.boxes:\n",
    "            x1 = box.x_min\n",
    "            x2 = box.x_max\n",
    "            y1 = box.y_min\n",
    "            y2 = box.y_max\n",
    "            box_list.append([x1, y1, x2, y2])\n",
    "            label_list.append(box.class_nb)\n",
    "\n",
    "        targets = [{\n",
    "            \"boxes\": torch.Tensor(box_list).to(DEVICE),\n",
    "            \"labels\": torch.Tensor(label_list).to(DEVICE)\n",
    "        }]\n",
    "\n",
    "        fcos(tensor_image, targets)\n",
    "        # fcos(tensor_image, [{\"boxes\": torch.Tensor([[x0, y0, x1, y1]]).to(DEVICE), \"labels\": torch.Tensor([1, 2]).to(DEVICE)}])\n",
    "\n",
    "\n",
    "\n",
    "################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tensorek = torch.Tensor([[0, 0, 4, 4]]).to(DEVICE)\n",
    "# x0, y0, x1, y1 = tensorek.unsqueeze(dim=0).unbind(dim=2)  # (1, M)\n",
    "# print(tensorek)\n",
    "# print(x0, y0, x1, y1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
